{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Anime Recommendation System\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def preProcess(pattern):\n",
    "    all_words = []\n",
    "    tags = []\n",
    "    xy = []\n",
    "    w = tokenize_without_numbers(pattern)\n",
    "    w2 = removal_of_stop_words(w)\n",
    "    w3 = stemming(w2)\n",
    "    # print(w3)\n",
    "    return w3\n",
    "\n",
    "\n",
    "def tokenize_without_numbers(text):\n",
    "    text = text.strip()\n",
    "    pattern = r'\\b[a-zA-Z]+\\b'  # Matches words containing only letters\n",
    "\n",
    "    # Use findall() to get all matching tokens\n",
    "    tokens = re.findall(pattern, text)\n",
    "    newTokens = []\n",
    "    for token in tokens:\n",
    "        newTokens.append(token.lower())\n",
    "    return newTokens\n",
    "\n",
    "\n",
    "stop_words = {'did', \"couldn't\", 'it', 'some', 'him', 'do', \"hadn't\", 'the', 'had', 'when', 'wouldn', \"hasn't\", 'doesn', 'he', 'how', 'once', 'this', 'been', 'also', 'an', \"it's\", 'under', 'if', 'which', 'be', 's', \"don't\", 'further', 'until', 'every', \"you'll\", 'our', 'only', 'move', 'didn', 'again', 'hasn', 'am', 'isn', 'are', \"you'd\", 'everything', 'because', 't', 'does', 'were', 'yours', 're', 'with', 'ourselves', 'she', 'where', 'such', 'hers', 'being', 'out', 'ma', 'i', 'too', 'ain', 'from', 'or', 'made', 'and', 'above', 'no', 'shan', 'll', 'mightn', \"should've\", 'If', 'a', 'other', 'himself', 'why', 'don', \"wasn't\", 'have', 'here', 'can', 'doing', 'up', 'yourselves', 'as', 'off', \"aren't\", 'mustn', 'itself', 'yourself', 'its', 'won', 'my', 'aren', 'at', 'that',\n",
    "              'than', 'shouldn', 'His', 'o', 'm', \"weren't\", 'whom', 'y', 'couldn', 'weren', \"mightn't\", 'for', 'just', \"you've\", 'ours', 'those', 'between', 'through', 'is', \"wouldn't\", 'these', 'any', \"won't\", 've', 'has', \"that'll\", 'about', \"doesn't\", 'against', 'hadn', 'you', 'most', 'while', 'after', \"she's\", 'his', 'into', 'who', 'on', 'during', 'will', 'they', 'by', 'below', 'in', \"shan't\", 'each', 'so', 'having', 'theirs', 'more', 'nor', 'few', 'we', 'then', \"didn't\", 'down', 'wasn', 'them', 'was', \"mustn't\", 'very', 'herself', 'not', \"isn't\", 'over', 'what', 'should', \"haven't\", 'themselves', 'myself', 'both', 'of', \"shouldn't\", 'someone', 'needn', \"you're\", 'their', 'now', \"needn't\", 'but', 'there', 'all', 'haven', 'her', 'same', 'me', 'your', 'before', 'to', 'd', 'own'}\n",
    "\n",
    "\n",
    "def removal_of_stop_words(words):\n",
    "    all_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stop_words:\n",
    "            all_words.append(w)\n",
    "\n",
    "    return all_words\n",
    "\n",
    "\n",
    "stemming_rules = [\n",
    "    (\"sses\", \"ss\"),\n",
    "    (\"ies\", \"i\"),\n",
    "    (\"ss\", \"ss\"),\n",
    "    (\"s\", \"\"),\n",
    "    (\"tions\", \"t\"),\n",
    "    (\"ative\", \"\"),\n",
    "    (\"atives\", \"\"),\n",
    "    (\"ize\", \"\"),\n",
    "    (\"izes\", \"ize\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"ized\", \"ize\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"ized\", \"ize\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"tive\", \"\"),\n",
    "    (\"tives\", \"\"),\n",
    "    (\"ic\", \"\"),\n",
    "    (\"ics\", \"\"),\n",
    "    (\"ical\", \"\"),\n",
    "    (\"ically\", \"\"),\n",
    "    (\"icity\", \"\"),\n",
    "    (\"ionize\", \"ion\"),\n",
    "    (\"ionizes\", \"ionize\"),\n",
    "    (\"ionizing\", \"ionize\"),\n",
    "    (\"ionized\", \"ionize\"),\n",
    "    (\"ional\", \"\"),\n",
    "    (\"ionally\", \"\"),\n",
    "    (\"ioning\", \"ion\"),\n",
    "    (\"ionings\", \"ioning\"),\n",
    "    (\"ioned\", \"ion\"),\n",
    "    (\"ioner\", \"ion\"),\n",
    "    (\"ioners\", \"ioner\"),\n",
    "    (\"ionable\", \"ion\"),\n",
    "    (\"ionables\", \"ionable\"),\n",
    "    (\"ioning\", \"ion\"),\n",
    "    (\"ionings\", \"ioning\"),\n",
    "    (\"ization\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izational\", \"ize\"),\n",
    "    (\"izationally\", \"ize\"),\n",
    "    (\"izationing\", \"ize\"),\n",
    "    (\"izationings\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izationed\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"ishly\", \"\"),\n",
    "    (\"ishness\", \"\"),\n",
    "    (\"ishnesses\", \"ishness\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"istic\", \"\"),\n",
    "    (\"istically\", \"\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"istically\", \"ist\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"istication\", \"ist\"),\n",
    "    (\"istications\", \"istication\"),\n",
    "    (\"isticated\", \"isticate\"),\n",
    "    (\"isticate\", \"ist\"),\n",
    "    (\"istically\", \"ist\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"isticatedly\", \"isticate\"),\n",
    "    (\"isticatedness\", \"isticate\"),\n",
    "    (\"istication\", \"isticate\"),\n",
    "    (\"istications\", \"isticate\"),\n",
    "    (\"evening\", \"evening\"),\n",
    "    (\"morning\", \"morning\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"isms\", \"ism\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ists\", \"ist\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ists\", \"ist\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ally\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ally\", \"\"),\n",
    "    (\"ed\", \"\"),\n",
    "    (\"ing\", \"\"),\n",
    "    (\"er\", \"\"),\n",
    "    (\"or\", \"\"),\n",
    "    (\"ar\", \"\"),\n",
    "    (\"ary\", \"\"),\n",
    "    (\"ery\", \"\"),\n",
    "    (\"ful\", \"\"),\n",
    "    (\"less\", \"\"),\n",
    "    (\"ness\", \"\"),\n",
    "    (\"ship\", \"\"),\n",
    "    (\"sion\", \"\"),\n",
    "    (\"tion\", \"t\"),\n",
    "    (\"ive\", \"\"),\n",
    "    (\"ize\", \"\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"ized\", \"ize\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ally\", \"\"),\n",
    "    (\"ed\", \"\"),\n",
    "    (\"ing\", \"\"),\n",
    "    (\"er\", \"\"),\n",
    "    (\"or\", \"\"),\n",
    "    (\"ar\", \"\"),\n",
    "    (\"ary\", \"\"),\n",
    "    (\"ery\", \"\"),\n",
    "    (\"ful\", \"\"),\n",
    "    (\"less\", \"\"),\n",
    "    (\"ness\", \"\"),\n",
    "    (\"ship\", \"\"),\n",
    "    (\"sion\", \"\"),\n",
    "    (\"tion\", \"t\"),\n",
    "    (\"ive\", \"\"),\n",
    "    (\"ize\", \"\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"ized\", \"ize\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"ise\", \"\"),\n",
    "    (\"ises\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"ise\", \"\"),\n",
    "    (\"ises\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"ise\", \"\"),\n",
    "    (\"ises\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"ise\", \"\"),\n",
    "    (\"ises\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"ise\", \"\"),\n",
    "    (\"ises\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"tive\", \"\"),\n",
    "    (\"tives\", \"\"),\n",
    "    (\"ic\", \"\"),\n",
    "    (\"ics\", \"\"),\n",
    "    (\"ical\", \"\"),\n",
    "    (\"ically\", \"\"),\n",
    "    (\"icity\", \"\"),\n",
    "    (\"ionize\", \"ion\"),\n",
    "    (\"ionizes\", \"ionize\"),\n",
    "    (\"ionizing\", \"ionize\"),\n",
    "    (\"ionized\", \"ionize\"),\n",
    "    (\"ional\", \"\"),\n",
    "    (\"ionally\", \"\"),\n",
    "    (\"ioning\", \"ion\"),\n",
    "    (\"ionings\", \"ioning\"),\n",
    "    (\"ioned\", \"ion\"),\n",
    "    (\"ioner\", \"ion\"),\n",
    "    (\"ioners\", \"ioner\"),\n",
    "    (\"ionable\", \"ion\"),\n",
    "    (\"ionables\", \"ionable\"),\n",
    "    (\"ioning\", \"ion\"),\n",
    "    (\"ionings\", \"ioning\"),\n",
    "    (\"ization\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izational\", \"ize\"),\n",
    "    (\"izationally\", \"ize\"),\n",
    "    (\"izationing\", \"ize\"),\n",
    "    (\"izationings\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izationed\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ishly\", \"\"),\n",
    "    (\"ishness\", \"\"),\n",
    "    (\"ishnesses\", \"ishness\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"istic\", \"\"),\n",
    "    (\"istically\", \"\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"istically\", \"ist\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"istication\", \"ist\"),\n",
    "    (\"istications\", \"istication\"),\n",
    "    (\"isticated\", \"isticate\"),\n",
    "    (\"isticate\", \"ist\"),\n",
    "    (\"istically\", \"ist\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"isticatedly\", \"isticate\"),\n",
    "    (\"isticatedness\", \"isticate\"),\n",
    "    (\"isticatednesses\", \"isticatedness\"),\n",
    "    (\"istication\", \"isticate\"),\n",
    "    (\"istications\", \"isticate\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"isms\", \"ism\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ists\", \"ist\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ists\", \"ist\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ational\", \"ate\"),\n",
    "    (\"tional\", \"tion\"),\n",
    "    (\"enci\", \"ence\"),\n",
    "    (\"anci\", \"ance\"),\n",
    "    (\"izer\", \"ize\"),\n",
    "    (\"alli\", \"al\"),\n",
    "    (\"entli\", \"ent\"),\n",
    "    (\"eli\", \"e\"),\n",
    "    (\"ousli\", \"ous\"),\n",
    "    (\"ization\", \"ize\"),\n",
    "    (\"ation\", \"ate\"),\n",
    "    (\"ator\", \"ate\"),\n",
    "    (\"alism\", \"al\"),\n",
    "    (\"iveness\", \"ive\"),\n",
    "    (\"fulness\", \"ful\"),\n",
    "    (\"ousness\", \"ous\"),\n",
    "    (\"aliti\", \"al\"),\n",
    "    (\"iviti\", \"ive\"),\n",
    "    (\"biliti\", \"ble\"),\n",
    "    (\"icate\", \"ic\"),\n",
    "    (\"ative\", \"\"),\n",
    "    (\"alize\", \"al\"),\n",
    "    (\"iciti\", \"ic\"),\n",
    "    (\"ical\", \"ic\"),\n",
    "    (\"ful\", \"\"),\n",
    "    (\"ness\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ance\", \"\"),\n",
    "    (\"ence\", \"\"),\n",
    "    (\"er\", \"\"),\n",
    "    (\"ic\", \"\"),\n",
    "    (\"able\", \"\"),\n",
    "    (\"ible\", \"\"),\n",
    "    (\"ant\", \"\"),\n",
    "    (\"ement\", \"\"),\n",
    "    (\"ment\", \"\"),\n",
    "    (\"ent\", \"\"),\n",
    "    (\"ou\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"iti\", \"\"),\n",
    "    (\"ous\", \"\"),\n",
    "    (\"ive\", \"\"),\n",
    "    (\"ize\", \"\"),\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "def stemming(all_words):\n",
    "    stemmed_word = []\n",
    "    for w in all_words:\n",
    "        for rules in stemming_rules:\n",
    "            suffix, replace = rules\n",
    "            if w.endswith(suffix):\n",
    "                w = w[:-len(suffix)] + replace\n",
    "        stemmed_word.append(w)\n",
    "    return stemmed_word\n",
    "\n",
    "# preProcess(\"By default, a function must be called with the correct number of arguments. Meaning that if your function expects 2 arguments, you have to call the function with 2 arguments, not more, and not less.\")\n",
    "\n",
    "\n",
    "inverted_index = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('main dataset\\\\main.csv')\n",
    "userData = pd.read_csv('main dataset\\\\user_profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def count_string_occurrences(string, array_of_strings):\n",
    "    count = 0\n",
    "    for string_in_array in array_of_strings:\n",
    "        if string_in_array == string:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "inverted_index = defaultdict(dict)\n",
    "terms = []\n",
    "tokenindex = []\n",
    "\n",
    "\n",
    "def makeIndex():\n",
    "    if 'Description' in data.columns:\n",
    "        for index, row in data.iterrows():\n",
    "            document = str(row['Description'])\n",
    "            predoc = preProcess(document)\n",
    "            tokenindex.append(predoc)\n",
    "            terms = list(set(terms + predoc))\n",
    "\n",
    "    else:\n",
    "        print(\"The 'text' column does not exist in the CSV file. Please check the column name.\")\n",
    "\n",
    "    with open(\"index.txt\", \"w\") as file:\n",
    "        for term in terms:\n",
    "            di = []\n",
    "            i = 0\n",
    "            for i in range(len(tokenindex)):\n",
    "                if term in tokenindex[i]:\n",
    "                    # tf=tf.get(term,0)+1\n",
    "                    docid = str(i+1)\n",
    "                    tf = count_string_occurrences(term, tokenindex[i])\n",
    "                    di.append(\"({}, {})\".format(str(docid), str(tf)))\n",
    "                    # print(i)\n",
    "\n",
    "            # inverted_index[term] = di\n",
    "            ditf = \"{} -> {}\\n\".format(term, \", \".join(map(str, di)))\n",
    "            file.write(ditf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Indexfile and Make Index Map**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "total_doc = 0\n",
    "indexmap = 0\n",
    "\n",
    "\n",
    "def makeindexmap():\n",
    "    global total_doc\n",
    "    term_doc_freq_map = defaultdict(dict)\n",
    "\n",
    "    with open('index.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            term, doc_freq_part = map(str.strip, line.split('->'))\n",
    "            doc_freq_pairs = [pair.strip('()')\n",
    "                              for pair in doc_freq_part.split('), ')]\n",
    "\n",
    "            for pair in doc_freq_pairs:\n",
    "                parts = pair.strip('()').split(', ')\n",
    "\n",
    "                try:\n",
    "                    doc_id, freq = map(int, parts)\n",
    "                    term_doc_freq_map[term][doc_id] = freq\n",
    "\n",
    "                    if total_doc < doc_id:\n",
    "                        total_doc = doc_id\n",
    "                except ValueError:\n",
    "                    print(f\"Error parsing pair: {pair}\")\n",
    "    # print(total_doc)\n",
    "    return term_doc_freq_map\n",
    "\n",
    "\n",
    "indexmap = makeindexmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weight Index**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def weigthcal(indexmap, Stringarr):\n",
    "    weighta = {}\n",
    "    for token in Stringarr:\n",
    "        if token in weighta:\n",
    "            continue\n",
    "        else:\n",
    "            tfa = 1+math.log(count_string_occurrences(token, Stringarr), 2)\n",
    "            idfa = math.log(total_doc/(len(indexmap.get(token, {}))+1), 2)\n",
    "            weighta[token] = tfa*idfa\n",
    "\n",
    "    return weighta\n",
    "\n",
    "\n",
    "def weightindex(qweight, indexmap):\n",
    "    weightindex = {}\n",
    "    for token in qweight:\n",
    "        if token in indexmap:\n",
    "            idf = math.log(total_doc/(len(indexmap.get(token, {}))+1), 2)\n",
    "            for key in indexmap[token]:\n",
    "                tf = 1+math.log(indexmap[token][key], 2)\n",
    "                weightindex.setdefault(key, {})\n",
    "                weightindex[key][token] = tf*idf\n",
    "\n",
    "    return weightindex\n",
    "\n",
    "\n",
    "def Uweightindex(indexmap):\n",
    "    weightindex = defaultdict(dict)\n",
    "\n",
    "    for token, doc_dict in indexmap.items():\n",
    "        idf = math.log(total_doc / (len(doc_dict) + 1), 2)\n",
    "\n",
    "        for key, tf in doc_dict.items():\n",
    "            tf_weight = 1 + math.log(tf, 2)\n",
    "            # tf_weight=tf\n",
    "            weightindex[key][token] = tf_weight * idf\n",
    "\n",
    "    return weightindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity Function and Magnitude of Vector**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def getMagnitude(vector):\n",
    "    return np.linalg.norm(list(vector.values()))\n",
    "\n",
    "\n",
    "def Similarity(qweight, weightindex):\n",
    "    weightsimilarity = {}\n",
    "\n",
    "    qMagnitude = getMagnitude(qweight)\n",
    "\n",
    "    for key, doc_vector in weightindex.items():\n",
    "        weightMagnitude = getMagnitude(doc_vector)\n",
    "        print(key)\n",
    "\n",
    "        if weightMagnitude == 0 or qMagnitude == 0:\n",
    "            weightsimilarity[key] = 0\n",
    "        else:\n",
    "            common_tokens = set(qweight.keys()) & set(doc_vector.keys())\n",
    "            q_vector = np.array([qweight[token] for token in common_tokens])\n",
    "            doc_vector = np.array([doc_vector[token]\n",
    "                                  for token in common_tokens])\n",
    "\n",
    "            weightsimilarity[key] = np.dot(\n",
    "                doc_vector, q_vector) / (weightMagnitude * qMagnitude)\n",
    "\n",
    "    return dict(sorted(weightsimilarity.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge Documents**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mergedoc_user(read, uweight):\n",
    "    weight = {}\n",
    "    for doc in read:\n",
    "        for token in uweight[doc]:\n",
    "            if token in weight:\n",
    "                weight[token] += uweight[doc][token]\n",
    "            else:\n",
    "                weight[token] = uweight[doc][token]\n",
    "\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Document from UserID**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "\n",
    "def extract_watch_or_read(user_id, csv_file):\n",
    "    watch_or_read_list = []\n",
    "\n",
    "    with open(csv_file, 'r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if int(row['User ID']) == user_id:\n",
    "                watch_or_read_list = [\n",
    "                    int(item) for item in ast.literal_eval(row['watch or read'])]\n",
    "                break\n",
    "\n",
    "    return watch_or_read_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Name of Doc**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_document_names(csv_file, doc_ids):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {csv_file}\")\n",
    "        return None\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: Empty DataFrame - {csv_file}\")\n",
    "        return None\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Unable to parse CSV - {csv_file}\")\n",
    "        return None\n",
    "\n",
    "    if 'ID' not in df.columns:\n",
    "        print(\"Error: 'ID' column not found in the DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    filtered_df = df[df['ID'].isin(doc_ids)]\n",
    "\n",
    "    id_to_name = dict(zip(filtered_df['ID'], filtered_df['Title']))\n",
    "\n",
    "    return id_to_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching and Ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "\n",
    "\n",
    "def userRecommendation(read):\n",
    "    uweight = Uweightindex(indexmap)\n",
    "    qweigth = mergedoc_user(read, uweight)\n",
    "    sim = Similarity(qweigth, uweight)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def searchQuery(query):\n",
    "    queryToken = preProcess(query)\n",
    "    qweigth = weigthcal(indexmap, queryToken)\n",
    "    weightindexa = weightindex(qweigth, indexmap)\n",
    "    sim = Similarity(qweigth, weightindexa)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def searchAndRank(query, userID):\n",
    "    if query == \"\":\n",
    "        csv_file_path = 'main dataset//user_profiles.csv'\n",
    "        read = extract_watch_or_read(userID, csv_file_path)\n",
    "        start_time = time.time()\n",
    "        sim = userRecommendation(read)\n",
    "        end_time = time.time()\n",
    "    else:\n",
    "        start_time = time.time()\n",
    "        sim = searchQuery(query)\n",
    "        end_time = time.time()\n",
    "    top_10 = dict(list(sim.items())[:10])\n",
    "    print(top_10)\n",
    "\n",
    "    csv_file_path_main = 'main dataset//main.csv'\n",
    "    doc_names_dict = get_document_names(csv_file_path_main, top_10)\n",
    "\n",
    "    for doc_id, similarity_score in top_10.items():\n",
    "        doc_name = doc_names_dict.get(doc_id, \"Name not found\")\n",
    "\n",
    "        print(\n",
    "            f'Doc ID: {doc_id}, Name: {doc_name}, Similarity: {similarity_score}')\n",
    "\n",
    "    print(f\"Total time for Searching is {end_time-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# darray=[]\n",
    "# darray=[{\"x\",\"y\"}]\n",
    "# darray.append({\"z\",\"k\"})\n",
    "# print(darray)\n",
    "# Str=\"Hello my name is Swastik Mukati. How are you?\"\n",
    "# print(inverted_index(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def askUser():\n",
    "#     print(\"\\nAre you an existing User? (Yes/No)\")\n",
    "#     x = input().lower()\n",
    "#     if x in [\"yes\", \"y\"]:\n",
    "#         id = existingUser()\n",
    "#     else:\n",
    "#         id = guest()\n",
    "#     return id\n",
    "\n",
    "\n",
    "# def existingUser():\n",
    "#     print(\"\\nEnter User ID\")\n",
    "#     id = input()\n",
    "#     if id == \"1\":\n",
    "#         print(\"\\nIt is Guest ID\")\n",
    "#         guest()\n",
    "#     return id\n",
    "\n",
    "\n",
    "# def guest():\n",
    "#     id = \"1\"\n",
    "#     return id\n",
    "\n",
    "\n",
    "# def menu(id):\n",
    "#     # print(\"\\nUser id is\")\n",
    "#     print(\"\\nUser Id : \"+ id)\n",
    "#     print(\"\\nEnter \\n1) Enter Description of the anime/manga that you'd like to read \\n2) Get recommendations based on the previous User's watched list\")\n",
    "#     x = input()\n",
    "#     if x == \"1\":\n",
    "#         searchBasedOnDescription()\n",
    "#     elif x == \"2\":\n",
    "#         basedOnPrevious(id)\n",
    "#     else:\n",
    "#         print(\"\\nInvalid Input\")\n",
    "#         menu(id)\n",
    "\n",
    "\n",
    "# def searchBasedOnDescription():\n",
    "#     print(\"\\nEnter Description of the Anime/Manga:\")\n",
    "#     desc = input()\n",
    "#     search(desc)\n",
    "\n",
    "\n",
    "# def basedOnPrevious(id):\n",
    "#     searchByID(id)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     id = askUser()\n",
    "#     print(id)\n",
    "#     menu(id)\n",
    "\n",
    "# def search(desc):\n",
    "#     print(\"\\nSearching for anime/manga with description: \" + desc)\n",
    "\n",
    "# def searchByID(id):\n",
    "#     print(\"\\nSearching for anime/manga for User ID: \" + id)\n",
    "\n",
    "\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('main dataset//main.csv')\n",
    "\n",
    "feedback_ids = [1,2,3,4,5,6,7,8,9,10,23,24,20]\n",
    "for feedback_id in feedback_ids:\n",
    "    if feedback_id in data['ID'].values:\n",
    "        data.loc[data['ID'] == feedback_id, 'Feedback'] = 1\n",
    "        data.loc[data['ID'] == feedback_id, 'Evaluation'] = data.loc[data['ID'] ==feedback_id, 'Feedback'] * data.loc[data['ID'] == feedback_id, 'Rating']\n",
    "        data.loc[data['ID'] == feedback_id, 'Normalized Evaluation'] = data.loc[data['ID']== feedback_id, 'Evaluation'] / 87316*100\n",
    "data.to_csv('main dataset//main.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('main dataset//main.csv')\n",
    "\n",
    "df['Evaluation'] = df['Rating'] * df['Feedback']\n",
    "df['Normalized Evaluation'] = df['Evaluation'] / 87316 * 100\n",
    "\n",
    "\n",
    "df.to_csv('main dataset//main.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userId = 2\n",
    "# query = \"\"\n",
    "# pageNumber = 1\n",
    "def QueryGenerator(userId, query):\n",
    "    if query == '':\n",
    "        data = pd.read_csv('main dataset\\\\main.csv')\n",
    "        userData = pd.read_csv('main dataset\\\\user_profiles.csv')\n",
    "        userData['watch or read'] = userData['watch or read'].apply(eval)\n",
    "\n",
    "        def get_union_of_descriptions(user_id):\n",
    "            user_watch_ids = userData.loc[userData['User ID']\n",
    "                                          == user_id, 'watch or read'].values[0]\n",
    "            user_rows = data[data['ID'].astype(str).isin(user_watch_ids)]\n",
    "            descriptions = user_rows['Description']\n",
    "            union_of_descriptions = ' '.join(descriptions)\n",
    "            return union_of_descriptions\n",
    "\n",
    "        query = get_union_of_descriptions(userId)\n",
    "    return query\n",
    "# print(f\"Query for User ID {userId}:\\n{query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spelling correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "\n",
    "\n",
    "def correct_spelling(text):\n",
    "    english_dict = enchant.Dict(\"en_US\")\n",
    "\n",
    "    words = text.split()\n",
    "\n",
    "    corrected_words = [english_dict.suggest(\n",
    "        word)[0] if not english_dict.check(word) else word for word in words]\n",
    "\n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId = 4\n",
    "query = \"\"\n",
    "pageNumber = 1\n",
    "query = correct_spelling(query)\n",
    "query = QueryGenerator(userId, query)\n",
    "searchAndRank(query, userId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'main dataset\\\\main.csv'\n",
    "with open(data, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    data = list(reader)\n",
    "\n",
    "descriptions = [row[2] for row in data]\n",
    "ids = [row[0] for row in data]\n",
    "\n",
    "index_path = 'index.txt'\n",
    "with open(index_path, 'r', encoding='utf-8') as file:\n",
    "    inverted_index = {}\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' -> ')\n",
    "        if len(parts) >= 2:\n",
    "            term = parts[0]\n",
    "            postings = [tuple(map(int, pair.strip('()').split(', ')))\n",
    "                        for pair in parts[1].split('), (')]\n",
    "            inverted_index[term] = postings\n",
    "\n",
    "user_query = query\n",
    "user_query = re.findall(r'\\b\\w+\\b', user_query.lower())\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions + [' '.join(user_query)])\n",
    "\n",
    "cosine_similarities = cosine_similarity(\n",
    "    tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "\n",
    "ranking = np.argsort(cosine_similarities)[::-1]\n",
    "\n",
    "print(\"\\nTop 10 Results:\")\n",
    "for i in range(10):\n",
    "    index = ranking[i]\n",
    "    print(\n",
    "        f\"{i + 1}. ID: {ids[index]}, Title: {data[index][1]}, Cosine Similarity: {cosine_similarities[index]:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity + normalized evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = 'main dataset\\\\main.csv'\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    data = list(reader)\n",
    "\n",
    "descriptions = [row[2] for row in data]\n",
    "ids = [row[0] for row in data]\n",
    "normalized_evaluations = [float(row[6]) for row in data]\n",
    "\n",
    "user_query = query\n",
    "\n",
    "user_query = re.findall(r'\\b\\w+\\b', user_query.lower())\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions + [' '.join(user_query)])\n",
    "\n",
    "\n",
    "cosine_similarities = cosine_similarity(\n",
    "    tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "\n",
    "\n",
    "cosine_similarities = np.append(cosine_similarities, [0])\n",
    "normalized_evaluations = np.append(normalized_evaluations, [0])\n",
    "\n",
    "\n",
    "combined_scores = np.add(cosine_similarities, normalized_evaluations)\n",
    "\n",
    "\n",
    "ranking = np.argsort(combined_scores)[::-1]\n",
    "\n",
    "\n",
    "print(\"\\nTop 10 Results:\")\n",
    "for i in range(10):\n",
    "    index = ranking[i]\n",
    "    print(\n",
    "        f\"{i + 1}. ID: {ids[index]}, Title: {data[index][1]}, Combined Score: {combined_scores[index]:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered Ranking and page changing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'main dataset\\\\main.csv'\n",
    "user_profiles_path = 'main dataset\\\\user_profiles.csv'\n",
    "page_number = pageNumber\n",
    "results_per_page = 10\n",
    "\n",
    "\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    data = list(reader)\n",
    "\n",
    "\n",
    "descriptions = [row[2] for row in data]\n",
    "ids = [row[0] for row in data]\n",
    "normalized_evaluations = [float(row[6]) for row in data]\n",
    "\n",
    "\n",
    "with open(user_profiles_path, 'r', encoding='utf-8') as user_file:\n",
    "    user_reader = csv.reader(user_file)\n",
    "    next(user_reader)\n",
    "    user_data = list(user_reader)\n",
    "\n",
    "\n",
    "user_id_to_exclude = userId\n",
    "watched_or_read = set()\n",
    "for user_row in user_data:\n",
    "    if user_row[0] == user_id_to_exclude and user_row[2] != '[]':\n",
    "        watched_or_read = set(eval(user_row[2]))\n",
    "\n",
    "\n",
    "def get_watch_or_read(user_id):\n",
    "    with open(user_profiles_path, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['User ID'] == str(user_id):\n",
    "                watch_or_read_str = row['watch or read']\n",
    "                watch_or_read_list = eval(watch_or_read_str.replace(\n",
    "                    \"'\", \"\")) if watch_or_read_str else []\n",
    "                return watch_or_read_list\n",
    "    return None\n",
    "\n",
    "\n",
    "x = get_watch_or_read(user_id_to_exclude)\n",
    "ids_to_exclude = set(map(str, x))\n",
    "\n",
    "\n",
    "filtered_data = [row for row in data if row[0]\n",
    "                 not in watched_or_read and row[0] not in ids_to_exclude]\n",
    "\n",
    "\n",
    "descriptions = [row[2] for row in filtered_data]\n",
    "ids = [row[0] for row in filtered_data]\n",
    "normalized_evaluations = [float(row[6]) for row in filtered_data]\n",
    "\n",
    "user_query = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions + [' '.join(user_query)])\n",
    "\n",
    "cosine_similarities = cosine_similarity(\n",
    "    tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "cosine_similarities = np.append(cosine_similarities, [0])\n",
    "normalized_evaluations = np.append(normalized_evaluations, [0])\n",
    "\n",
    "\n",
    "combined_scores = np.add(cosine_similarities, normalized_evaluations)\n",
    "ranking = np.argsort(combined_scores)[::-1]\n",
    "\n",
    "\n",
    "start_index = (page_number - 1) * results_per_page\n",
    "end_index = start_index + results_per_page\n",
    "\n",
    "print(f\"\\nTop {results_per_page} Results (Page {page_number}):\")\n",
    "for i in range(start_index, end_index):\n",
    "    if i >= len(ranking):\n",
    "        break\n",
    "    index = ranking[i]\n",
    "    print(f\"{i + 1}. ID: {ids[index]},Combined Score: {combined_scores[index]:.10f},  Title: {filtered_data[index][1]}, Description: {filtered_data[index][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'main dataset\\\\main.csv'\n",
    "user_profiles_path = 'main dataset\\\\user_profiles.csv'\n",
    "page_number = pageNumber\n",
    "results_per_page = 10\n",
    "\n",
    "\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    data = list(reader)\n",
    "\n",
    "descriptions = [row[2] for row in data]\n",
    "ids = [row[0] for row in data]\n",
    "normalized_evaluations = [float(row[6]) for row in data]\n",
    "\n",
    "\n",
    "with open(user_profiles_path, 'r', encoding='utf-8') as user_file:\n",
    "    user_reader = csv.reader(user_file)\n",
    "    next(user_reader)\n",
    "    user_data = list(user_reader)\n",
    "\n",
    "\n",
    "user_id_to_exclude = userId\n",
    "watched_or_read = set()\n",
    "for user_row in user_data:\n",
    "    if user_row[0] == user_id_to_exclude and user_row[2] != '[]':\n",
    "        watched_or_read = set(eval(user_row[2]))\n",
    "\n",
    "\n",
    "def get_watch_or_read(user_id):\n",
    "    with open(user_profiles_path, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['User ID'] == str(user_id):\n",
    "                watch_or_read_str = row['watch or read']\n",
    "                watch_or_read_list = eval(watch_or_read_str.replace(\n",
    "                    \"'\", \"\")) if watch_or_read_str else []\n",
    "                return watch_or_read_list\n",
    "    return None\n",
    "\n",
    "\n",
    "x = get_watch_or_read(user_id_to_exclude)\n",
    "ids_to_exclude = set(map(str, x))\n",
    "\n",
    "filtered_data = [row for row in data if row[0]\n",
    "                 not in watched_or_read and row[0] not in ids_to_exclude]\n",
    "\n",
    "descriptions = [row[2] for row in filtered_data]\n",
    "ids = [row[0] for row in filtered_data]\n",
    "normalized_evaluations = [float(row[6]) for row in filtered_data]\n",
    "\n",
    "user_query = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions + [' '.join(user_query)])\n",
    "\n",
    "cosine_similarities = cosine_similarity(\n",
    "    tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "cosine_similarities = np.append(cosine_similarities, [0])\n",
    "normalized_evaluations = np.append(normalized_evaluations, [0])\n",
    "\n",
    "combined_scores = np.add(cosine_similarities, normalized_evaluations)\n",
    "ranking = np.argsort(combined_scores)[::-1]\n",
    "\n",
    "start_index = (page_number - 1) * results_per_page\n",
    "end_index = start_index + results_per_page\n",
    "\n",
    "print(f\"\\nTop {results_per_page} Results (Page {page_number}):\")\n",
    "for i in range(start_index, end_index):\n",
    "    if i >= len(ranking):\n",
    "        break\n",
    "    index = ranking[i]\n",
    "    print(f\"{i + 1}. ID: {ids[index]}, Combined Score: {combined_scores[index]:.10f},Title: {filtered_data[index][1]}, , Description: {filtered_data[index][2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
