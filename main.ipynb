{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Anime Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(pattern):\n",
    "    all_words = []\n",
    "    tags = []\n",
    "    xy = []\n",
    "    w =tokenize_without_numbers(pattern)\n",
    "    w2 =removal_of_stop_words(w)\n",
    "    w3 =stemming(w2)\n",
    "    # print(w3)\n",
    "    return w3\n",
    "\n",
    "\n",
    "def tokenize_without_numbers(text):\n",
    "    text=text.strip()\n",
    "    pattern = r'\\b[a-zA-Z]+\\b'  # Matches words containing only letters\n",
    "\n",
    "    # Use findall() to get all matching tokens\n",
    "    tokens = re.findall(pattern, text)\n",
    "    newTokens = []\n",
    "    for token in tokens:\n",
    "        newTokens.append(token.lower())\n",
    "    return newTokens\n",
    "\n",
    "\n",
    "stop_words = {'did', \"couldn't\", 'it', 'some', 'him', 'do', \"hadn't\", 'the', 'had', 'when', 'wouldn', \"hasn't\", 'doesn', 'he', 'how', 'once', 'this', 'been', 'also', 'an', \"it's\", 'under', 'if', 'which', 'be', 's', \"don't\", 'further', 'until', 'every', \"you'll\", 'our', 'only', 'move', 'didn', 'again', 'hasn', 'am', 'isn', 'are', \"you'd\", 'everything', 'because', 't', 'does', 'were', 'yours', 're', 'with', 'ourselves', 'she', 'where', 'such', 'hers', 'being', 'out', 'ma', 'i', 'too', 'ain', 'from', 'or', 'made', 'and', 'above', 'no', 'shan', 'll', 'mightn', \"should've\", 'If', 'a', 'other', 'himself', 'why', 'don', \"wasn't\", 'have', 'here', 'can', 'doing', 'up', 'yourselves', 'as', 'off', \"aren't\", 'mustn', 'itself', 'yourself', 'its', 'won', 'my', 'aren', 'at', 'that', 'than', 'shouldn', 'His', 'o', 'm', \"weren't\", 'whom', 'y', 'couldn', 'weren', \"mightn't\", 'for', 'just', \"you've\", 'ours', 'those', 'between', 'through', 'is', \"wouldn't\", 'these', 'any', \"won't\", 've', 'has', \"that'll\", 'about', \"doesn't\", 'against', 'hadn', 'you', 'most', 'while', 'after', \"she's\", 'his', 'into', 'who', 'on', 'during', 'will', 'they', 'by', 'below', 'in', \"shan't\", 'each', 'so', 'having', 'theirs', 'more', 'nor', 'few', 'we', 'then', \"didn't\", 'down', 'wasn', 'them', 'was', \"mustn't\", 'very', 'herself', 'not', \"isn't\", 'over', 'what', 'should', \"haven't\", 'themselves', 'myself', 'both', 'of', \"shouldn't\", 'someone', 'needn', \"you're\", 'their', 'now', \"needn't\", 'but', 'there', 'all', 'haven', 'her', 'same', 'me', 'your', 'before', 'to', 'd', 'own'}\n",
    "\n",
    "def removal_of_stop_words(words):\n",
    "    all_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stop_words:\n",
    "            all_words.append(w)\n",
    "\n",
    "    return all_words\n",
    "\n",
    "stemming_rules = [\n",
    "    (\"sses\", \"ss\"),\n",
    "    (\"ies\", \"i\"),\n",
    "    (\"ss\", \"ss\"),\n",
    "    (\"s\", \"\"),\n",
    "    (\"tions\", \"t\"),\n",
    "    (\"ative\", \"\"),\n",
    "    (\"atives\", \"\"),\n",
    "    (\"ize\", \"\"),\n",
    "    (\"izes\", \"ize\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"ized\", \"ize\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"ized\", \"ize\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"tive\", \"\"),\n",
    "    (\"tives\", \"\"),\n",
    "    (\"ic\", \"\"),\n",
    "    (\"ics\", \"\"),\n",
    "    (\"ical\", \"\"),\n",
    "    (\"ically\", \"\"),\n",
    "    (\"icity\", \"\"),\n",
    "    (\"ionize\", \"ion\"),\n",
    "    (\"ionizes\", \"ionize\"),\n",
    "    (\"ionizing\", \"ionize\"),\n",
    "    (\"ionized\", \"ionize\"),\n",
    "    (\"ional\", \"\"),\n",
    "    (\"ionally\", \"\"),\n",
    "    (\"ioning\", \"ion\"),\n",
    "    (\"ionings\", \"ioning\"),\n",
    "    (\"ioned\", \"ion\"),\n",
    "    (\"ioner\", \"ion\"),\n",
    "    (\"ioners\", \"ioner\"),\n",
    "    (\"ionable\", \"ion\"),\n",
    "    (\"ionables\", \"ionable\"),\n",
    "    (\"ioning\", \"ion\"),\n",
    "    (\"ionings\", \"ioning\"),\n",
    "    (\"ization\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izational\", \"ize\"),\n",
    "    (\"izationally\", \"ize\"),\n",
    "    (\"izationing\", \"ize\"),\n",
    "    (\"izationings\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izationed\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"ishly\", \"\"),\n",
    "    (\"ishness\", \"\"),\n",
    "    (\"ishnesses\", \"ishness\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"istic\", \"\"),\n",
    "    (\"istically\", \"\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"istically\", \"ist\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"istication\", \"ist\"),\n",
    "    (\"istications\", \"istication\"),\n",
    "    (\"isticated\", \"isticate\"),\n",
    "    (\"isticate\", \"ist\"),\n",
    "    (\"istically\", \"ist\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"isticatedly\", \"isticate\"),\n",
    "    (\"isticatedness\", \"isticate\"),\n",
    "    (\"istication\", \"isticate\"),\n",
    "    (\"istications\", \"isticate\"),\n",
    "    (\"evening\", \"evening\"),\n",
    "    (\"morning\", \"morning\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"isms\", \"ism\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ists\", \"ist\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ists\", \"ist\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ally\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ally\", \"\"),\n",
    "    (\"ed\", \"\"),\n",
    "    (\"ing\", \"\"),\n",
    "    (\"er\", \"\"),\n",
    "    (\"or\", \"\"),\n",
    "    (\"ar\", \"\"),\n",
    "    (\"ary\", \"\"),\n",
    "    (\"ery\", \"\"),\n",
    "    (\"ful\", \"\"),\n",
    "    (\"less\", \"\"),\n",
    "    (\"ness\", \"\"),\n",
    "    (\"ship\", \"\"),\n",
    "    (\"sion\", \"\"),\n",
    "    (\"tion\", \"t\"),\n",
    "    (\"ive\", \"\"),\n",
    "    (\"ize\", \"\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"ized\", \"ize\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ally\", \"\"),\n",
    "    (\"ed\", \"\"),\n",
    "    (\"ing\", \"\"),\n",
    "    (\"er\", \"\"),\n",
    "    (\"or\", \"\"),\n",
    "    (\"ar\", \"\"),\n",
    "    (\"ary\", \"\"),\n",
    "    (\"ery\", \"\"),\n",
    "    (\"ful\", \"\"),\n",
    "    (\"less\", \"\"),\n",
    "    (\"ness\", \"\"),\n",
    "    (\"ship\", \"\"),\n",
    "    (\"sion\", \"\"),\n",
    "    (\"tion\", \"t\"),\n",
    "    (\"ive\", \"\"),\n",
    "    (\"ize\", \"\"),\n",
    "    (\"izing\", \"ize\"),\n",
    "    (\"ized\", \"ize\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"ise\", \"\"),\n",
    "    (\"ises\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"ise\", \"\"),\n",
    "    (\"ises\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"ise\", \"\"),\n",
    "    (\"ises\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"ise\", \"\"),\n",
    "    (\"ises\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"en\", \"\"),\n",
    "    (\"ify\", \"\"),\n",
    "    (\"ise\", \"\"),\n",
    "    (\"ises\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ising\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ised\", \"ise\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"tive\", \"\"),\n",
    "    (\"tives\", \"\"),\n",
    "    (\"ic\", \"\"),\n",
    "    (\"ics\", \"\"),\n",
    "    (\"ical\", \"\"),\n",
    "    (\"ically\", \"\"),\n",
    "    (\"icity\", \"\"),\n",
    "    (\"ionize\", \"ion\"),\n",
    "    (\"ionizes\", \"ionize\"),\n",
    "    (\"ionizing\", \"ionize\"),\n",
    "    (\"ionized\", \"ionize\"),\n",
    "    (\"ional\", \"\"),\n",
    "    (\"ionally\", \"\"),\n",
    "    (\"ioning\", \"ion\"),\n",
    "    (\"ionings\", \"ioning\"),\n",
    "    (\"ioned\", \"ion\"),\n",
    "    (\"ioner\", \"ion\"),\n",
    "    (\"ioners\", \"ioner\"),\n",
    "    (\"ionable\", \"ion\"),\n",
    "    (\"ionables\", \"ionable\"),\n",
    "    (\"ioning\", \"ion\"),\n",
    "    (\"ionings\", \"ioning\"),\n",
    "    (\"ization\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izational\", \"ize\"),\n",
    "    (\"izationally\", \"ize\"),\n",
    "    (\"izationing\", \"ize\"),\n",
    "    (\"izationings\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izationed\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"izations\", \"ize\"),\n",
    "    (\"ish\", \"\"),\n",
    "    (\"ishly\", \"\"),\n",
    "    (\"ishness\", \"\"),\n",
    "    (\"ishnesses\", \"ishness\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"istic\", \"\"),\n",
    "    (\"istically\", \"\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"istically\", \"ist\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"istication\", \"ist\"),\n",
    "    (\"istications\", \"istication\"),\n",
    "    (\"isticated\", \"isticate\"),\n",
    "    (\"isticate\", \"ist\"),\n",
    "    (\"istically\", \"ist\"),\n",
    "    (\"istical\", \"ist\"),\n",
    "    (\"isticatedly\", \"isticate\"),\n",
    "    (\"isticatedness\", \"isticate\"),\n",
    "    (\"isticatednesses\", \"isticatedness\"),\n",
    "    (\"istication\", \"isticate\"),\n",
    "    (\"istications\", \"isticate\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"isms\", \"ism\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ists\", \"ist\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ists\", \"ist\"),\n",
    "    (\"ist\", \"\"),\n",
    "    (\"ational\", \"ate\"),\n",
    "    (\"tional\", \"tion\"),\n",
    "    (\"enci\", \"ence\"),\n",
    "    (\"anci\", \"ance\"),\n",
    "    (\"izer\", \"ize\"),\n",
    "    (\"alli\", \"al\"),\n",
    "    (\"entli\", \"ent\"),\n",
    "    (\"eli\", \"e\"),\n",
    "    (\"ousli\", \"ous\"),\n",
    "    (\"ization\", \"ize\"),\n",
    "    (\"ation\", \"ate\"),\n",
    "    (\"ator\", \"ate\"),\n",
    "    (\"alism\", \"al\"),\n",
    "    (\"iveness\", \"ive\"),\n",
    "    (\"fulness\", \"ful\"),\n",
    "    (\"ousness\", \"ous\"),\n",
    "    (\"aliti\", \"al\"),\n",
    "    (\"iviti\", \"ive\"),\n",
    "    (\"biliti\", \"ble\"),\n",
    "    (\"icate\", \"ic\"),\n",
    "    (\"ative\", \"\"),\n",
    "    (\"alize\", \"al\"),\n",
    "    (\"iciti\", \"ic\"),\n",
    "    (\"ical\", \"ic\"),\n",
    "    (\"ful\", \"\"),\n",
    "    (\"ness\", \"\"),\n",
    "    (\"al\", \"\"),\n",
    "    (\"ance\", \"\"),\n",
    "    (\"ence\", \"\"),\n",
    "    (\"er\", \"\"),\n",
    "    (\"ic\", \"\"),\n",
    "    (\"able\", \"\"),\n",
    "    (\"ible\", \"\"),\n",
    "    (\"ant\", \"\"),\n",
    "    (\"ement\", \"\"),\n",
    "    (\"ment\", \"\"),\n",
    "    (\"ent\", \"\"),\n",
    "    (\"ou\", \"\"),\n",
    "    (\"ism\", \"\"),\n",
    "    (\"ate\", \"\"),\n",
    "    (\"iti\", \"\"),\n",
    "    (\"ous\", \"\"),\n",
    "    (\"ive\", \"\"),\n",
    "    (\"ize\", \"\"),\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "def stemming(all_words):\n",
    "    stemmed_word = []\n",
    "    for w in all_words:\n",
    "        for rules in stemming_rules:\n",
    "            suffix, replace = rules\n",
    "            if w.endswith(suffix):\n",
    "                w = w[:-len(suffix)] + replace\n",
    "        stemmed_word.append(w)\n",
    "    return stemmed_word\n",
    "\n",
    "# preProcess(\"By default, a function must be called with the correct number of arguments. Meaning that if your function expects 2 arguments, you have to call the function with 2 arguments, not more, and not less.\")\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "inverted_index = defaultdict(dict)\n",
    "\n",
    "# def invertedIndex():\n",
    "#     # Iterate through each document\n",
    "#     # for doc_id, document_content in enumerate(documents, 1):\n",
    "#         # Tokenize, remove stop words, and stem the document content\n",
    "#         terms = preProcess(\"By default, a function must be called with the correct number of arguments. Meaning that if your function expects 2 arguments, you have to call the function with 2 arguments, not more, and not less.\")\n",
    "\n",
    "#         # Count term frequency\n",
    "#         term_freq = {}\n",
    "#         for term in terms:\n",
    "#             term_freq[term] = term_freq.get(term, 0) + 1\n",
    "\n",
    "#         # Update the inverted index\n",
    "#         # for term, freq in term_freq.items():\n",
    "#         #     inverted_index[term][doc_id] = freq\n",
    "#         print(term_freq)\n",
    "\n",
    "# invertedIndex()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('main dataset\\\\main.csv')\n",
    "userData = pd.read_csv('main dataset\\\\user_profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def invertedIndex(document):\n",
    "#     terms = preProcess(document)\n",
    "#     term_freq = {}\n",
    "#     for term in terms:\n",
    "#         term_freq[term] = term_freq.get(term, 0) + 1\n",
    "#     return term_freq\n",
    "\n",
    "def count_string_occurrences(string, array_of_strings):\n",
    "  count = 0\n",
    "  for string_in_array in array_of_strings:\n",
    "    if string_in_array == string:\n",
    "      count += 1\n",
    "  return count\n",
    "\n",
    "inverted_index = defaultdict(dict)\n",
    "terms=[]\n",
    "tokenindex=[]\n",
    "def makeIndex():\n",
    "    if 'Description' in data.columns:\n",
    "        for index, row in data.iterrows():\n",
    "            document = str(row['Description'])\n",
    "            predoc=preProcess(document)\n",
    "            tokenindex.append(predoc)\n",
    "            terms= list(set(terms + predoc))\n",
    "            # if isinstance(document, str):\n",
    "            #     term_freq = invertedIndex(document)\n",
    "\n",
    "            #     for term, freq in term_freq.items():\n",
    "            #         inverted_index[term][index] = freq\n",
    "            # else:\n",
    "            #     # Handle non-string values (e.g., NaN or floats)\n",
    "            #     print(f\"Skipped row {index} due to non-string 'text' value.\")\n",
    "    else:\n",
    "        print(\"The 'text' column does not exist in the CSV file. Please check the column name.\")\n",
    "    # print(document)\n",
    "    # print(terms)\n",
    "    # print(tokenindex[98])\n",
    "    with open(\"index.txt\",\"w\") as file:\n",
    "        for term in terms:\n",
    "            di=[]\n",
    "            i=0\n",
    "            for i in range(len(tokenindex)):\n",
    "                if term in tokenindex[i]:\n",
    "                    # tf=tf.get(term,0)+1\n",
    "                    docid=str(i+1)\n",
    "                    tf=count_string_occurrences(term,tokenindex[i])\n",
    "                    di.append(\"({}, {})\".format(str(docid), str(tf)))\n",
    "                    # print(i)\n",
    "\n",
    "            # inverted_index[term] = di\n",
    "            ditf = \"{} -> {}\\n\".format(term, \", \".join(map(str, di)))\n",
    "            file.write(ditf)\n",
    "\n",
    "    # with open(\"index.txt\", \"w\") as file:\n",
    "    #     # Write content to the file\n",
    "    #     for term, di in inverted_index.items():\n",
    "    #         ditf = \"{} -> {}\\n\".format(term, \", \".join(map(str, di)))\n",
    "    #         file.write(ditf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Indexfile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample content of the text file\n",
    "file_content = \"term -> (103,2), (100,1)\"\n",
    "# def makeIndexMapping():\n",
    "#     with open ('newindex.txt', 'r') as file:\n",
    "#         for line in file:\n",
    "#             term_doc_freq_map={}\n",
    "#             term, doc_freq_part = map(str.strip, line.split('->'))\n",
    "#             doc_freq_pairs = doc_freq_part.strip('()').split(',')\n",
    "#             term_doc_freq_map[term]={'docmap':[]}\n",
    "#             for pair in doc_freq_pairs:\n",
    "#                 doc_freq_map={}\n",
    "#                 doc_id, freq = map(int, pair.strip('()').split(','))\n",
    "#                 doc_freq_map[doc_id]={'freq':[]}\n",
    "#                 doc_freq_map[doc_id]['freq'].append(freq)\n",
    "#                 term_doc_freq_map[term]['docmap'].append(doc_freq_map)\n",
    "#                 # term_doc_freq_map[doc_id]['freq'].append(freq)\n",
    "#             # for term, data in term_doc_freq_map.items():\n",
    "#             #     print(data['docmap'][0])\n",
    "\n",
    "#             for term, data in term_doc_freq_map.items():\n",
    "#                 print(f\"Term: {term}\")\n",
    "\n",
    "#                 docmaps = data['docmap']\n",
    "                \n",
    "#                 # Iterate through docmaps\n",
    "#                 for docmap in docmaps:\n",
    "#                     # Iterate through document IDs and frequencies\n",
    "#                     for doc_id, freq_data in docmap.items():\n",
    "#                         print(f\"  Doc ID: {doc_id}, Frequency: {freq_data['freq'][0]}\")\n",
    "\n",
    "# makeIndexMapping()\n",
    "\n",
    "\n",
    "\n",
    "def makeIndexMapping():\n",
    "    with open('newindex.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            term_doc_freq_map = {}\n",
    "            term, doc_freq_part = map(str.strip, line.split('->'))\n",
    "            # print(doc_freq_part)\n",
    "            # doc_freq_pairs = doc_freq_part.strip('()').split(', ')\n",
    "            doc_freq_pairs = [pair.strip('()') for pair in doc_freq_part.split('), ')]\n",
    "\n",
    "            term_doc_freq_map[term] = {'docmap': []}\n",
    "\n",
    "            for pair in doc_freq_pairs:\n",
    "                doc_freq_map = {}\n",
    "                parts = pair.strip('()').split(', ')\n",
    "                # Check if there are enough values to unpack\n",
    "                if len(parts) == 2:\n",
    "                    doc_id, freq = map(int, parts)\n",
    "                    doc_freq_map[doc_id] = {'freq': []}\n",
    "                    doc_freq_map[doc_id]['freq'].append(freq)\n",
    "                    term_doc_freq_map[term]['docmap'].append(doc_freq_map)\n",
    "                else:\n",
    "                    print(f\"Error parsing pair: {pair}\")\n",
    "\n",
    "            for term, data in term_doc_freq_map.items():\n",
    "                print(f\"Term: {term}\")\n",
    "\n",
    "                docmaps = data['docmap']\n",
    "                \n",
    "                # Iterate through docmaps\n",
    "                for docmap in docmaps:\n",
    "                    # Iterate through document IDs and frequencies\n",
    "                    for doc_id, freq_data in docmap.items():\n",
    "                        print(f\"  Doc ID: {doc_id}, Frequency: {freq_data['freq'][0]}\")\n",
    "\n",
    "# Call the function to execute the code\n",
    "makeIndexMapping()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top 10 rows of 'data' DataFrame:\")\n",
    "print(data.iloc[:11, 1:3])\n",
    "\n",
    "print(\"\\nTop 10 rows of 'userData' DataFrame:\")\n",
    "print(userData.head(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# darray=[]\n",
    "# darray=[{\"x\",\"y\"}]\n",
    "# darray.append({\"z\",\"k\"})\n",
    "# print(darray)\n",
    "# Str=\"Hello my name is Swastik Mukati. How are you?\"\n",
    "# print(inverted_index(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def askUser():\n",
    "#     print(\"\\nAre you an existing User? (Yes/No)\")\n",
    "#     x = input().lower()\n",
    "#     if x in [\"yes\", \"y\"]:\n",
    "#         id = existingUser()\n",
    "#     else:\n",
    "#         id = guest()\n",
    "#     return id\n",
    "\n",
    "\n",
    "# def existingUser():\n",
    "#     print(\"\\nEnter User ID\")\n",
    "#     id = input()\n",
    "#     if id == \"1\":\n",
    "#         print(\"\\nIt is Guest ID\")\n",
    "#         guest()\n",
    "#     return id\n",
    "\n",
    "\n",
    "# def guest():\n",
    "#     id = \"1\"\n",
    "#     return id\n",
    "\n",
    "\n",
    "# def menu(id):\n",
    "#     # print(\"\\nUser id is\")\n",
    "#     print(\"\\nUser Id : \"+ id)\n",
    "#     print(\"\\nEnter \\n1) Enter Description of the anime/manga that you'd like to read \\n2) Get recommendations based on the previous User's watched list\")\n",
    "#     x = input()\n",
    "#     if x == \"1\":\n",
    "#         searchBasedOnDescription()\n",
    "#     elif x == \"2\":\n",
    "#         basedOnPrevious(id)\n",
    "#     else:\n",
    "#         print(\"\\nInvalid Input\")\n",
    "#         menu(id)\n",
    "\n",
    "\n",
    "# def searchBasedOnDescription():\n",
    "#     print(\"\\nEnter Description of the Anime/Manga:\")\n",
    "#     desc = input()\n",
    "#     search(desc)\n",
    "\n",
    "\n",
    "# def basedOnPrevious(id):\n",
    "#     searchByID(id)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     id = askUser()\n",
    "#     print(id)\n",
    "#     menu(id)\n",
    "\n",
    "# def search(desc):\n",
    "#     print(\"\\nSearching for anime/manga with description: \" + desc)\n",
    "\n",
    "# def searchByID(id):\n",
    "#     print(\"\\nSearching for anime/manga for User ID: \" + id)\n",
    "\n",
    "\n",
    "# main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userId = 4\n",
    "query = \"\"\n",
    "pageNumber = 1\n",
    "if query == '':\n",
    "    data = pd.read_csv('main dataset\\\\main.csv')\n",
    "    userData = pd.read_csv('main dataset\\\\user_profiles.csv')\n",
    "    userData['watch or read'] = userData['watch or read'].apply(eval)\n",
    "\n",
    "    def get_union_of_descriptions(user_id):\n",
    "        user_watch_ids = userData.loc[userData['User ID']\n",
    "                                        == user_id, 'watch or read'].values[0]\n",
    "        user_rows = data[data['ID'].astype(str).isin(user_watch_ids)]\n",
    "        descriptions = user_rows['Description']\n",
    "        union_of_descriptions = ' '.join(descriptions)\n",
    "        return union_of_descriptions\n",
    "\n",
    "    query = get_union_of_descriptions(userId)\n",
    "    flag_query = 1\n",
    "else:\n",
    "    flag_query = 0\n",
    "\n",
    "print(f\"Query for User ID {userId}:\\n{query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "\n",
    "if flag_query == 0:\n",
    "    def correct_spelling(text):\n",
    "        english_dict = enchant.Dict(\"en_US\")\n",
    "\n",
    "        words = text.split()\n",
    "\n",
    "        corrected_words = [english_dict.suggest(\n",
    "            word)[0] if not english_dict.check(word) else word for word in words]\n",
    "\n",
    "        corrected_text = ' '.join(corrected_words)\n",
    "\n",
    "        return corrected_text\n",
    "\n",
    "    query = correct_spelling(query)\n",
    "\n",
    "print(\"Corrected Query:\", query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'main dataset\\\\main.csv'\n",
    "with open(data, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip header\n",
    "    data = list(reader)\n",
    "\n",
    "# Extract descriptions and IDs\n",
    "descriptions = [row[2] for row in data]\n",
    "ids = [row[0] for row in data]\n",
    "\n",
    "# Read the inverted index\n",
    "index_path = 'index.txt'\n",
    "with open(index_path, 'r', encoding='utf-8') as file:\n",
    "    inverted_index = {}\n",
    "    for line in file:\n",
    "        parts = line.strip().split(' -> ')\n",
    "        if len(parts) >= 2:\n",
    "            term = parts[0]\n",
    "            postings = [tuple(map(int, pair.strip('()').split(', ')))\n",
    "                        for pair in parts[1].split('), (')]\n",
    "            inverted_index[term] = postings\n",
    "\n",
    "# Process the user query\n",
    "user_query = query\n",
    "# Tokenize and convert to lowercase\n",
    "user_query = re.findall(r'\\b\\w+\\b', user_query.lower())\n",
    "\n",
    "# Calculate TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions + [' '.join(user_query)])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarities = cosine_similarity(\n",
    "    tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "\n",
    "# Rank descriptions based on cosine similarity\n",
    "ranking = np.argsort(cosine_similarities)[::-1]\n",
    "\n",
    "# Print top 10 results with ID\n",
    "print(\"\\nTop 10 Results:\")\n",
    "for i in range(10):\n",
    "    index = ranking[i]\n",
    "    print(\n",
    "        f\"{i + 1}. ID: {ids[index]}, Title: {data[index][1]}, Cosine Similarity: {cosine_similarities[index]:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity + normalized evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_path = 'main dataset\\\\main.csv'\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    data = list(reader)\n",
    "\n",
    "descriptions = [row[2] for row in data]\n",
    "ids = [row[0] for row in data]\n",
    "normalized_evaluations = [float(row[6]) for row in data]\n",
    "\n",
    "user_query = query\n",
    "\n",
    "user_query = re.findall(r'\\b\\w+\\b', user_query.lower())\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions + [' '.join(user_query)])\n",
    "\n",
    "\n",
    "cosine_similarities = cosine_similarity(\n",
    "    tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "\n",
    "\n",
    "cosine_similarities = np.append(cosine_similarities, [0])\n",
    "normalized_evaluations = np.append(normalized_evaluations, [0])\n",
    "\n",
    "\n",
    "combined_scores = np.add(cosine_similarities, normalized_evaluations)\n",
    "\n",
    "\n",
    "ranking = np.argsort(combined_scores)[::-1]\n",
    "\n",
    "\n",
    "print(\"\\nTop 10 Results:\")\n",
    "for i in range(10):\n",
    "    index = ranking[i]\n",
    "    print(\n",
    "        f\"{i + 1}. ID: {ids[index]}, Title: {data[index][1]}, Combined Score: {combined_scores[index]:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtered Ranking and page changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'main dataset\\\\main.csv'\n",
    "user_profiles_path = 'main dataset\\\\user_profiles.csv'\n",
    "page_number = pageNumber\n",
    "results_per_page = 10\n",
    "\n",
    "\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    data = list(reader)\n",
    "\n",
    "\n",
    "descriptions = [row[2] for row in data]\n",
    "ids = [row[0] for row in data]\n",
    "normalized_evaluations = [float(row[6]) for row in data]\n",
    "\n",
    "\n",
    "with open(user_profiles_path, 'r', encoding='utf-8') as user_file:\n",
    "    user_reader = csv.reader(user_file)\n",
    "    next(user_reader)\n",
    "    user_data = list(user_reader)\n",
    "\n",
    "\n",
    "user_id_to_exclude = userId\n",
    "watched_or_read = set()\n",
    "for user_row in user_data:\n",
    "    if user_row[0] == user_id_to_exclude and user_row[2] != '[]':\n",
    "        watched_or_read = set(eval(user_row[2]))\n",
    "\n",
    "\n",
    "\n",
    "def get_watch_or_read(user_id):\n",
    "    with open(user_profiles_path, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['User ID'] == str(user_id):\n",
    "                watch_or_read_str = row['watch or read']\n",
    "                watch_or_read_list = eval(watch_or_read_str.replace(\n",
    "                    \"'\", \"\")) if watch_or_read_str else []\n",
    "                return watch_or_read_list\n",
    "    return None\n",
    "\n",
    "\n",
    "x = get_watch_or_read(user_id_to_exclude)\n",
    "ids_to_exclude = set(map(str, x))\n",
    "\n",
    "\n",
    "filtered_data = [row for row in data if row[0]\n",
    "                 not in watched_or_read and row[0] not in ids_to_exclude]\n",
    "\n",
    "\n",
    "descriptions = [row[2] for row in filtered_data]\n",
    "ids = [row[0] for row in filtered_data]\n",
    "normalized_evaluations = [float(row[6]) for row in filtered_data]\n",
    "\n",
    "user_query = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions + [' '.join(user_query)])\n",
    "\n",
    "cosine_similarities = cosine_similarity(\n",
    "    tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "cosine_similarities = np.append(cosine_similarities, [0])\n",
    "normalized_evaluations = np.append(normalized_evaluations, [0])\n",
    "\n",
    "\n",
    "combined_scores = np.add(cosine_similarities, normalized_evaluations)\n",
    "ranking = np.argsort(combined_scores)[::-1]\n",
    "\n",
    "\n",
    "start_index = (page_number - 1) * results_per_page\n",
    "end_index = start_index + results_per_page\n",
    "\n",
    "print(f\"\\nTop {results_per_page} Results (Page {page_number}):\")\n",
    "for i in range(start_index, end_index):\n",
    "    if i >= len(ranking):\n",
    "        break\n",
    "    index = ranking[i]\n",
    "    print(f\"{i + 1}. ID: {ids[index]}, Title: {filtered_data[index][1]}, Combined Score: {combined_scores[index]:.10f}, Description: {filtered_data[index][2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'main dataset\\\\main.csv'\n",
    "user_profiles_path = 'main dataset\\\\user_profiles.csv'\n",
    "page_number = pageNumber\n",
    "results_per_page = 10\n",
    "\n",
    "# Read main dataset\n",
    "with open(dataset_path, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    data = list(reader)\n",
    "\n",
    "# Extract relevant data\n",
    "descriptions = [row[2] for row in data]\n",
    "ids = [row[0] for row in data]\n",
    "normalized_evaluations = [float(row[6]) for row in data]\n",
    "\n",
    "# Read user profiles\n",
    "with open(user_profiles_path, 'r', encoding='utf-8') as user_file:\n",
    "    user_reader = csv.reader(user_file)\n",
    "    next(user_reader)\n",
    "    user_data = list(user_reader)\n",
    "\n",
    "# Extract watched or read data for a specific user\n",
    "user_id_to_exclude = userId\n",
    "watched_or_read = set()\n",
    "for user_row in user_data:\n",
    "    if user_row[0] == user_id_to_exclude and user_row[2] != '[]':\n",
    "        watched_or_read = set(eval(user_row[2]))\n",
    "\n",
    "# Function to get watch or read data for a given user_id\n",
    "\n",
    "\n",
    "def get_watch_or_read(user_id):\n",
    "    with open(user_profiles_path, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            if row['User ID'] == str(user_id):\n",
    "                watch_or_read_str = row['watch or read']\n",
    "                watch_or_read_list = eval(watch_or_read_str.replace(\n",
    "                    \"'\", \"\")) if watch_or_read_str else []\n",
    "                return watch_or_read_list\n",
    "    return None\n",
    "\n",
    "\n",
    "# Get watch or read data for the user to exclude\n",
    "x = get_watch_or_read(user_id_to_exclude)\n",
    "ids_to_exclude = set(map(str, x))\n",
    "\n",
    "# Filter data based on watched_or_read and ids_to_exclude\n",
    "filtered_data = [row for row in data if row[0]\n",
    "                 not in watched_or_read and row[0] not in ids_to_exclude]\n",
    "\n",
    "# Extract relevant data from filtered data\n",
    "descriptions = [row[2] for row in filtered_data]\n",
    "ids = [row[0] for row in filtered_data]\n",
    "normalized_evaluations = [float(row[6]) for row in filtered_data]\n",
    "\n",
    "# Process user query\n",
    "user_query = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "\n",
    "# Vectorize descriptions and user query\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(descriptions + [' '.join(user_query)])\n",
    "\n",
    "# Calculate cosine similarities\n",
    "cosine_similarities = cosine_similarity(\n",
    "    tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "cosine_similarities = np.append(cosine_similarities, [0])\n",
    "normalized_evaluations = np.append(normalized_evaluations, [0])\n",
    "\n",
    "# Combine scores\n",
    "combined_scores = np.add(cosine_similarities, normalized_evaluations)\n",
    "ranking = np.argsort(combined_scores)[::-1]\n",
    "\n",
    "# Pagination\n",
    "start_index = (page_number - 1) * results_per_page\n",
    "end_index = start_index + results_per_page\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nTop {results_per_page} Results (Page {page_number}):\")\n",
    "for i in range(start_index, end_index):\n",
    "    if i >= len(ranking):\n",
    "        break\n",
    "    index = ranking[i]\n",
    "    print(f\"{i + 1}. ID: {ids[index]}, Title: {filtered_data[index][1]}, Combined Score: {combined_scores[index]:.10f}, Description: {filtered_data[index][2]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
